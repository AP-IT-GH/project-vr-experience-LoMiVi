behaviors:
  WeepingAngelBehavior:
    trainer_type: ppo
    hyperparameters:
      batch_size: 1024
      buffer_size: 10240
      learning_rate: 0.0003       # 3.0e-4
      beta: 0.005                # 5.0e-3 (entropy regularization, encourages exploration)
      epsilon: 0.2               # PPO clipping parameter
      lambd: 0.95                # GAE lambda
      num_epoch: 3               # Number of passes through buffer
      learning_rate_schedule: linear # or constant

    network_settings:
      normalize: true            # Normalize vector observations
      hidden_units: 256          # Number of units in hidden layers
      num_layers: 2              # Number of hidden layers
      vis_encode_type: simple    # For Raycasts, 'simple' is usually fine. 'resnet' if you add camera sensor.

    reward_signals:
      extrinsic:
        gamma: 0.99              # Discount factor for future rewards
        strength: 1.0

    keep_checkpoints: 5
    max_steps: 5000000           # Start with 5 million, increase if needed
    time_horizon: 128            # How many steps of experience to collect per agent before adding to buffer
    summary_freq: 50000          # How often to write summaries to TensorBoard
    threaded: true               # Use multi-threading for faster training if CPU allows

# Optional: Environment parameters if you want to vary things like moveSpeed during training curriculum
# environment_parameters:
#   agent_move_speed:
#     sampler_type: uniform
#     sampler_parameters:
#       min_value: 1.0
#       max_value: 3.0